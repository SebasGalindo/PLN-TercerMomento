import streamlit as st

st.title("Explicaci√≥n Detallada del Chatbot Financiero UdeC ü§ñüí°")
st.markdown("""
¬°Bienvenido a la explicaci√≥n t√©cnica de nuestro Chatbot Financiero!
Este proyecto ha sido refactorizado para utilizar la potencia de los Modelos de Lenguaje de Gran Escala (LLM)
a trav√©s de la API de Gemini de Google, combinada con un modelo de Machine Learning (LightGBM)
para ofrecer un an√°lisis preliminar de la salud financiera de una empresa.
""")

st.header("üéØ Objetivo del Proyecto")
st.markdown("""
El objetivo principal es permitir a un usuario ingresar informaci√≥n financiera clave de su empresa
a trav√©s de una interfaz conversacional amigable. El chatbot gu√≠a al usuario, recolecta los datos
necesarios, y finalmente presenta una clasificaci√≥n del nivel econ√≥mico de la empresa junto con
recomendaciones generales.

Esta versi√≥n reemplaza los modelos BERT utilizados anteriormente por la API de Gemini para
lograr una interacci√≥n m√°s natural y una extracci√≥n de informaci√≥n m√°s flexible y robusta.
""")

st.header("üìã Requisitos Funcionales Clave")
st.markdown("""
El chatbot debe ser capaz de:
* Mantener una conversaci√≥n fluida y amigable.
* Extraer datos como: nombre de la empresa, √°rea de actividad, n√∫mero de empleados, ingresos/activos, cartera y deudas.
* Intentar obtener el nombre del usuario.
* Inferir el sector econ√≥mico (Primario, Secundario, Terciario, Cuaternario) basado en el √°rea de actividad.
* Confirmar los datos extra√≠dos con el usuario.
* Utilizar un modelo LightGBM pre-entrenado para clasificar el nivel econ√≥mico.
* Presentar la clasificaci√≥n y consejos adaptados.
* Permitir iniciar un nuevo an√°lisis.
""")

st.header("üõ†Ô∏è Arquitectura y Flujo de Trabajo")
st.markdown("""
La soluci√≥n se divide en un **Frontend** (interfaz de usuario con Streamlit) y un **Backend** (l√≥gica del chatbot).

**Flujo General:**
1.  El usuario interact√∫a con el Frontend (`chatbot_streamlit.py`).
2.  El Frontend inicializa una sesi√≥n de chat con el Backend (`Backend/chatbot_logic.py`).
3.  El Backend crea y configura una sesi√≥n de chat con la API de Gemini (`google.genai`), especificando:
    * Una **instrucci√≥n de sistema** detallada (desde `Data/base_chatbot_instructions.txt`) que gu√≠a el comportamiento, tono y formato de respuesta de Gemini.
    * Un **esquema de respuesta JSON** (definido con clases Pydantic) para asegurar que Gemini devuelva la informaci√≥n de manera estructurada.
4.  Cuando el usuario env√≠a un mensaje, el Frontend lo pasa al Backend.
5.  El Backend env√≠a el mensaje a la API de Gemini a trav√©s de la sesi√≥n de chat activa.
6.  Gemini procesa el mensaje, considerando el historial de la conversaci√≥n, la instrucci√≥n del sistema y el esquema de respuesta. Devuelve un string JSON.
7.  El Backend recibe este string, lo limpia (si es necesario, ej: quita ```json ```) y lo parsea a un diccionario Python usando `json.loads()`.
8.  El Backend devuelve este diccionario al Frontend.
9.  El Frontend utiliza la informaci√≥n del diccionario (ej: `message_for_user`, `conversation_state`) para actualizar la interfaz y mostrar la respuesta del chatbot.
10. El ciclo se repite. El `conversation_state` devuelto por Gemini se utiliza para rastrear los campos pendientes y completados.
11. Cuando Gemini indica que todos los datos est√°n completos (`interaction_type: "datos_completos"`), el Backend toma el `conversation_state`, prepara los datos (incluyendo conversi√≥n a COP e inferencia de sector si es necesario) y los pasa al modelo LightGBM.
12. El modelo LightGBM predice la categor√≠a del nivel econ√≥mico.
13. El Backend genera un mensaje final con esta clasificaci√≥n y consejos relevantes.
14. El Frontend muestra este an√°lisis final.
""")

with st.expander("‚öôÔ∏è Profundizando en el Backend (`Backend/chatbot_logic.py`)", expanded=False):
    st.subheader("1. Inicializaci√≥n y Dependencias (`_initialize_client_and_dependencies`)")
    st.markdown("""
    Esta funci√≥n se ejecuta una vez al cargar el m√≥dulo y se encarga de:
    * **Cargar API Key de Gemini**: Lee las credenciales desde `Data/credentials.json`.
        * **Crear Cliente `google.genai.Client`**: Instancia el cliente para interactuar con la API de Gemini.
            ```python
            # En _initialize_client_and_dependencies()
            with open(CREDENTIALS_PATH, 'r') as f: credentials = json.load(f)
            API_KEY = credentials.get("API_KEY")
            genai_client_instance = genai.Client(api_key=API_KEY)
            ```
        * **Cargar Instrucci√≥n del Sistema**: Lee el prompt principal desde `Data/base_chatbot_instructions.txt`. Este archivo es VITAL, ya que define la personalidad, el flujo, los datos a recolectar y las reglas de formato JSON para Gemini.
        * **Cargar Modelo LightGBM**: Usa `joblib` para cargar el modelo de clasificaci√≥n (`.joblib`), el codificador de etiquetas y la lista de caracter√≠sticas esperadas.
        * **Obtener Tasas de Cambio**: Intenta obtener tasas de cambio actualizadas (USD/COP, EUR/COP) usando `yfinance`. Si falla, utiliza valores por defecto.
        """)

    st.subheader("2. Definici√≥n del Esquema de Respuesta con Pydantic")
    st.markdown("""
    Se definen varias clases Pydantic (`UpdatedField`, `CompletedField`, `ConversationState`, `ChatbotResponse`) que describen la estructura exacta del JSON que esperamos que la API de Gemini devuelva.
    La clase principal es `ChatbotResponse`. Estas clases **no se usan para instanciar objetos en Python despu√©s de recibir el JSON del API**, sino que **se pasan a la configuraci√≥n de la API de Gemini como `response_schema`**.
    Esto instruye a Gemini para que formatee su salida de acuerdo a este esquema.

        ```python
        # Ejemplo de la clase principal para el response_schema
        class ChatbotResponse(BaseModel):
            interaction_type: str
            message_for_user: str
            updated_field: UpdatedField = Field(default_factory=UpdatedField)
            next_question_key: Optional[str] = Field(None)
            conversation_state: ConversationState = Field(default_factory=ConversationState)
        ```
        """)

    st.subheader("3. Creaci√≥n de la Sesi√≥n de Chat con Gemini (`create_new_chat`)")
    st.markdown("""
    Esta funci√≥n es llamada por el Frontend para iniciar una nueva conversaci√≥n.
    * Utiliza `genai_client_instance.chats.create(model=model_name, config=...)`.
    * La parte m√°s importante es el par√°metro `config`, donde se pasa un objeto `google_genai_types.GenerateContentConfig`. Este objeto contiene:
        * `system_instruction`: El prompt detallado cargado previamente.
            * `response_mime_type="application/json"`: Le dice a Gemini que la respuesta debe ser un JSON.
            * `response_schema=ChatbotResponse`: Le pasa la clase Pydantic para que Gemini formatee el JSON seg√∫n esa estructura. ¬°Esta es la clave para obtener JSONs estructurados y fiables!
        * Devuelve un objeto de sesi√≥n de chat que mantiene el historial de la conversaci√≥n.

        ```python
        # Dentro de create_new_chat()
        chat_session = genai_client_instance.chats.create(
            model="gemini-2.0-flash", # o el modelo especificado
            config=google_genai_types.GenerateContentConfig(
                system_instruction=SYSTEM_INSTRUCTION_FINASSISTENTE,
                response_mime_type="application/json",
                response_schema=ChatbotResponse, # Especifica la estructura de salida
            )
        )
        return chat_session
        ```
        """)

    st.subheader("4. Env√≠o de Mensajes y Procesamiento de Respuestas (`send_message_to_chat`)")
    st.markdown("""
    Gestiona cada turno de la conversaci√≥n.
    * Recibe la sesi√≥n de chat activa y el mensaje del usuario.
    * Llama a `chat_session.send_message(user_message)`.
    * Accede a `api_response.text` para obtener el string JSON devuelto por Gemini (gracias a la configuraci√≥n del chat).
        * Limpia el string JSON (remueve ```json ``` si est√°n presentes).
        * Parsea el string a un diccionario Python usando `json.loads()`.
        * Realiza una validaci√≥n m√≠nima para asegurar que el diccionario contiene campos clave esperados.
        * **Devuelve el diccionario Python directamente**, junto con un mensaje de error si algo fall√≥.

        ```python
        # Dentro de send_message_to_chat()
        api_response = chat_session.send_message(user_message)
        # ... limpieza de cleaned_json_text ...
        parsed_dict_response = json.loads(cleaned_json_text)
        # ... validaci√≥n m√≠nima del dict ...
        return parsed_dict_response, None
        ```
        """)

    st.subheader("5. L√≥gica de Negocio Auxiliar")
    st.markdown("""
    * **`_convert_to_cop`**: Convierte valores monetarios de USD o EUR a Pesos Colombianos (COP) usando las tasas de cambio. Es crucial para normalizar los datos antes del an√°lisis LightGBM.
    * **`run_lgbm_analysis`**:
        * Se activa cuando Gemini indica que todos los datos necesarios han sido recolectados (`interaction_type: "datos_completos"`).
        * Recibe el `conversation_state` (como diccionario) de la √∫ltima respuesta de Gemini.
            * Transforma la lista de `completed_fields` en un formato m√°s manejable (un diccionario mapeando nombre de campo a su objeto/valor).
            * **Inferencia de Sector**: Verifica si el campo `sector` fue inferido por Gemini (seg√∫n la instrucci√≥n del sistema). Si no (o es "Desconocido"), realiza una llamada "one-shot" a `genai_client_instance.models.generate_content(...)` con un prompt espec√≠fico para clasificar el `area_categoria` en Primario, Secundario, Terciario o Cuaternario.
            * Prepara un DataFrame de Pandas con los datos num√©ricos (convertidos a COP) y categ√≥ricos.
            * Asegura que las columnas y tipos de datos coincidan con lo que el modelo LightGBM espera.
            * Utiliza el `lgbm_model` cargado para predecir el nivel econ√≥mico.
            * Devuelve la etiqueta de la categor√≠a predicha (ej: "Estable / Regular", "Cr√≠tica / Muy D√©bil").
        * **`generate_final_message_with_advice`**:
            * Toma la categor√≠a predicha por LightGBM y el nombre de la empresa.
            * Construye un mensaje de resumen formateado en Markdown, presentando la clasificaci√≥n y ofreciendo una serie de consejos generales y accionables adaptados a cada nivel econ√≥mico posible.
        """)

with st.expander("üé® Explorando el Frontend (`chatbot_streamlit.py`)", expanded=False):
    st.subheader("1. Configuraci√≥n General de la Aplicaci√≥n Streamlit")
    st.markdown("""
    * **`st.set_page_config`**: Define el t√≠tulo de la pesta√±a del navegador, el √≠cono y el layout (centrado o ancho).
    * **`st.title` y `st.caption`**: Muestran el t√≠tulo principal y un subt√≠tulo en la p√°gina.
    * **Importaci√≥n del Backend**: Intenta importar el m√≥dulo `Backend.chatbot_logic` como `bot`. Si hay un error, lo muestra y detiene la aplicaci√≥n.
    """)

    st.subheader("2. Inicializaci√≥n y Gesti√≥n del Estado de la Sesi√≥n (`st.session_state`)")
    st.markdown("""
    Streamlit utiliza `st.session_state` para mantener informaci√≥n entre interacciones del usuario y reruns de la p√°gina.
    * **`initialize_new_chat_session()`**:
        * Llamada al inicio y cuando se presiona "Nuevo Chat".
        * Invoca a `bot.create_new_chat()` para obtener una nueva `gemini_chat_session` y la guarda en `st.session_state.gemini_chat_session`.
        * Reinicia `st.session_state.chat_history` (una lista para los mensajes).
        * `st.session_state.chatbot_state` (que almacenar√° el diccionario `conversation_state` de Gemini) se pone a `None` inicialmente.
        * `st.session_state.analysis_done` se pone a `False`.
        * A√±ade un mensaje de bienvenida fijo al `chat_history` para que el usuario vea algo al iniciar.
        * **Bot√≥n "Nuevo Chat"**: Ubicado en la `st.sidebar`, llama a `initialize_new_chat_session()` y luego a `st.rerun()` para refrescar la interfaz.

        ```python
        # Ejemplo de inicializaci√≥n de nuevo chat en el frontend
        if st.sidebar.button("‚ú® Nuevo Chat", key="new_chat_button"):
            initialize_new_chat_session()
            st.rerun()

        if 'gemini_chat_session' not in st.session_state:
            initialize_new_chat_session()
        ```
        """)

    st.subheader("3. Interfaz de Chat")
    st.markdown("""
    * **Mostrar Historial (`st.chat_message`, `st.markdown`)**: Itera sobre `st.session_state.chat_history`. Cada mensaje se muestra dentro de un `st.chat_message(role)` (donde `role` es "user" o "assistant") y se renderiza con `st.markdown()` para permitir formato.
    * **Entrada del Usuario (`st.chat_input`)**: Proporciona el campo de texto para que el usuario escriba. Se deshabilita si `st.session_state.analysis_done` es `True`.

    ```python
        # Mostrar mensajes
        for message in st.session_state.chat_history:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        # Input del usuario
        user_prompt = st.chat_input("Escribe tu respuesta aqu√≠...", disabled=user_input_disabled)
        ```
        """)

    st.subheader("4. Flujo de Interacci√≥n con el Backend")
    st.markdown("""
    Cuando el usuario ingresa un mensaje (`user_prompt`):
    1.  El mensaje del usuario se a√±ade al `chat_history` y se muestra.
    2.  Se muestra un `st.spinner("FinAsistente est√° pensando... üß†")`.
    3.  Se llama a `bot.send_message_to_chat(st.session_state.gemini_chat_session, user_prompt)`.
    4.  Se recibe `response_dict, error_msg` del backend.
    5.  **Manejo de Respuesta/Error**:
        * Si hay `error_msg` (o `response_dict` es `None`), se muestra un mensaje de error en la interfaz.
        * Si no hay error, se extrae `response_dict.get("message_for_user")` y se muestra como la respuesta del chatbot. Este mensaje se a√±ade tambi√©n al `chat_history`.
        * Se actualiza `st.session_state.chatbot_state = response_dict.get("conversation_state")`.
    6.  **Verificaci√≥n de Fin de Conversaci√≥n y An√°lisis**:
        * Se comprueba si `response_dict.get("interaction_type") == "datos_completos"`.
        * Si es as√≠, se marca `st.session_state.analysis_done = True`.
        * Se muestra otro spinner indicando "Realizando an√°lisis financiero final... üìä".
        * Se llama a `bot.run_lgbm_analysis(st.session_state.chatbot_state)` (pasando el diccionario `conversation_state`).
        * Se obtiene el nombre de la empresa del `chatbot_state` para personalizar.
        * Se llama a `bot.generate_final_message_with_advice()` para obtener el mensaje de an√°lisis.
        * Este mensaje final se muestra con `st.markdown()` y se a√±ade al historial.
        * Se invoca `st.rerun()` para actualizar la UI y deshabilitar el campo de input.
    """)

st.markdown("---")
st.markdown("Este chatbot representa una integraci√≥n de tecnolog√≠as de IA para un prop√≥sito pr√°ctico, combinando la flexibilidad conversacional de los LLMs con la precisi√≥n predictiva de modelos de Machine Learning tradicionales.")
